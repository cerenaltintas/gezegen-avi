"""
ğŸ§  AI Explainability (AÃ§Ä±klanabilir AI) ModÃ¼lÃ¼
"Model neden bu kararÄ± verdi?" sorusuna detaylÄ± cevaplar

Bu modÃ¼l SHAP, LIME ve diÄŸer XAI tekniklerini iÃ§erir.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import shap
from lime import lime_tabular
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')


class ExplainabilityEngine:
    """
    AI Explainability motoru - Modelin kararlarÄ±nÄ± aÃ§Ä±klar
    """
    
    def __init__(self, model, scaler, feature_names, explainer=None):
        """
        Args:
            model: EÄŸitilmiÅŸ XGBoost modeli
            scaler: Veri Ã¶lÃ§ekleyici
            feature_names: Ã–zellik isimleri
            explainer: SHAP explainer (opsiyonel)
        """
        self.model = model
        self.scaler = scaler
        self.feature_names = list(feature_names)
        self.explainer = explainer
        
    def generate_decision_explanation(self, X_scaled, X_original, prediction_result):
        """
        Modelin kararÄ± iÃ§in kapsamlÄ± aÃ§Ä±klama oluÅŸtur
        
        Returns:
            Dict: AÃ§Ä±klama bilgileri
        """
        explanation = {
            'prediction': prediction_result['is_exoplanet'],
            'confidence': prediction_result['confidence'],
            'probability': prediction_result['probability_exoplanet'],
            'shap_analysis': self._get_shap_analysis(X_scaled),
            'feature_contribution': self._get_feature_contribution(X_scaled),
            'decision_rules': self._extract_decision_rules(X_original),
            'similar_cases': self._find_similar_cases(X_scaled),
            'what_if_analysis': self._what_if_analysis(X_original),
            'confidence_factors': self._analyze_confidence_factors(X_scaled, prediction_result)
        }
        
        return explanation
    
    def _get_shap_analysis(self, X_scaled):
        """SHAP deÄŸerleri ve analiz"""
        if self.explainer is None:
            return None
            
        try:
            shap_values = self.explainer.shap_values(X_scaled)
            
            # En etkili Ã¶zellikler
            shap_importance = np.abs(shap_values[0])
            top_indices = np.argsort(shap_importance)[::-1][:10]
            
            analysis = {
                'values': shap_values[0],
                'base_value': self.explainer.expected_value,
                'top_features': [
                    {
                        'name': self.feature_names[i],
                        'shap_value': float(shap_values[0][i]),
                        'contribution': float(shap_importance[i]),
                        'direction': 'positive' if shap_values[0][i] > 0 else 'negative',
                        'rank': rank + 1
                    }
                    for rank, i in enumerate(top_indices)
                ],
                'total_positive_impact': float(np.sum(shap_values[0][shap_values[0] > 0])),
                'total_negative_impact': float(np.sum(shap_values[0][shap_values[0] < 0]))
            }
            
            return analysis
        except Exception as e:
            print(f"SHAP analizi hatasÄ±: {e}")
            return None
    
    def _get_feature_contribution(self, X_scaled):
        """Her Ã¶zelliÄŸin katkÄ±sÄ±nÄ± hesapla"""
        try:
            # Model Ã¶zellik Ã¶nemleri
            feature_importance = self.model.feature_importances_
            
            # Her Ã¶zelliÄŸin deÄŸeri ile aÄŸÄ±rlÄ±klÄ± katkÄ±sÄ±
            contributions = []
            for i, fname in enumerate(self.feature_names):
                value = float(X_scaled[0][i])
                importance = float(feature_importance[i])
                weighted_contribution = value * importance
                
                contributions.append({
                    'feature': fname,
                    'value': value,
                    'importance': importance,
                    'weighted_contribution': weighted_contribution,
                    'percentage': importance * 100
                })
            
            # KatkÄ±ya gÃ¶re sÄ±rala
            contributions = sorted(contributions, key=lambda x: abs(x['weighted_contribution']), reverse=True)
            
            return {
                'contributions': contributions[:15],
                'total_importance': float(np.sum(feature_importance))
            }
        except Exception as e:
            print(f"Ã–zellik katkÄ± analizi hatasÄ±: {e}")
            return None
    
    def _extract_decision_rules(self, X_original):
        """Karar kurallarÄ±nÄ± Ã§Ä±kar"""
        rules = []
        
        try:
            # Kritik eÅŸik deÄŸerleri
            thresholds = {
                'koi_period': {'low': 1, 'high': 500, 'optimal': (5, 100)},
                'koi_prad': {'low': 0.5, 'high': 20, 'optimal': (1, 4)},
                'koi_teq': {'low': 200, 'high': 3000, 'optimal': (250, 400)},
                'koi_model_snr': {'low': 7, 'high': 1000, 'optimal': (15, 100)},
                'koi_depth': {'low': 50, 'high': 10000, 'optimal': (100, 2000)}
            }
            
            for feature, limits in thresholds.items():
                if feature in X_original.columns:
                    value = float(X_original[feature].iloc[0])
                    
                    rule = {
                        'feature': feature,
                        'value': value,
                        'threshold_low': limits['low'],
                        'threshold_high': limits['high'],
                        'optimal_range': limits['optimal'],
                        'status': 'optimal' if limits['optimal'][0] <= value <= limits['optimal'][1] 
                                 else 'acceptable' if limits['low'] <= value <= limits['high']
                                 else 'out_of_range',
                        'impact': 'positive' if limits['optimal'][0] <= value <= limits['optimal'][1] else 'negative'
                    }
                    rules.append(rule)
            
            return rules
        except Exception as e:
            print(f"Karar kuralÄ± Ã§Ä±karma hatasÄ±: {e}")
            return []
    
    def _find_similar_cases(self, X_scaled):
        """Benzer durumlarÄ± bul (simÃ¼lasyon)"""
        # Not: GerÃ§ek uygulamada veritabanÄ±ndan benzer Ã¶rnekler Ã§ekilir
        return {
            'similar_count': 0,
            'note': 'Bu Ã¶zellik gelecekte eklenecek'
        }
    
    def _what_if_analysis(self, X_original):
        """Ne olurdu analizi - kritik Ã¶zelliklerde deÄŸiÅŸiklik"""
        scenarios = []
        
        try:
            critical_features = ['koi_period', 'koi_prad', 'koi_depth', 'koi_model_snr']
            
            for feature in critical_features:
                if feature in X_original.columns:
                    original_value = float(X_original[feature].iloc[0])
                    
                    # %20 artÄ±ÅŸ ve azalÄ±ÅŸ senaryolarÄ±
                    scenarios.append({
                        'feature': feature,
                        'original_value': original_value,
                        'scenarios': [
                            {'change': '+20%', 'new_value': original_value * 1.2},
                            {'change': '-20%', 'new_value': original_value * 0.8},
                            {'change': '+50%', 'new_value': original_value * 1.5},
                            {'change': '-50%', 'new_value': original_value * 0.5}
                        ]
                    })
            
            return scenarios
        except Exception as e:
            print(f"What-if analizi hatasÄ±: {e}")
            return []
    
    def _analyze_confidence_factors(self, X_scaled, prediction_result):
        """GÃ¼ven faktÃ¶rlerini analiz et"""
        try:
            confidence = prediction_result['confidence']
            probability = prediction_result['probability_exoplanet']
            
            factors = {
                'confidence_level': confidence,
                'probability_margin': abs(probability - 0.5),
                'factors': []
            }
            
            # YÃ¼ksek gÃ¼ven faktÃ¶rleri
            if confidence > 0.9:
                factors['factors'].append({
                    'factor': 'YÃ¼ksek GÃ¼ven',
                    'description': 'Model Ã§ok emin',
                    'impact': 'positive'
                })
            elif confidence < 0.7:
                factors['factors'].append({
                    'factor': 'DÃ¼ÅŸÃ¼k GÃ¼ven',
                    'description': 'Model belirsiz',
                    'impact': 'negative'
                })
            
            # OlasÄ±lÄ±k marjÄ±
            if abs(probability - 0.5) > 0.4:
                factors['factors'].append({
                    'factor': 'Net Karar',
                    'description': 'OlasÄ±lÄ±k aÃ§Ä±kÃ§a bir tarafa eÄŸilimli',
                    'impact': 'positive'
                })
            
            return factors
        except Exception as e:
            print(f"GÃ¼ven faktÃ¶rÃ¼ analizi hatasÄ±: {e}")
            return None


def create_decision_narrative(explanation: Dict) -> str:
    """
    Karar iÃ§in anlatÄ±sal aÃ§Ä±klama oluÅŸtur
    
    Returns:
        str: Ä°nsan tarafÄ±ndan okunabilir aÃ§Ä±klama
    """
    narrative_parts = []
    
    # Tahmin sonucu
    decision = "Ã¶tegezegen" if explanation['prediction'] else "Ã¶tegezegen deÄŸil"
    confidence = explanation['confidence'] * 100
    
    narrative_parts.append(f"ğŸ” **Karar:** Bu aday '{decision}' olarak sÄ±nÄ±flandÄ±rÄ±ldÄ±.")
    narrative_parts.append(f"ğŸ“Š **GÃ¼ven:** Model bu kararda %{confidence:.1f} gÃ¼ven dÃ¼zeyine sahip.")
    
    # SHAP analizi
    if explanation['shap_analysis']:
        shap = explanation['shap_analysis']
        top_feature = shap['top_features'][0]
        
        narrative_parts.append(f"\nğŸ§  **Ana Etken:** '{top_feature['name']}' Ã¶zelliÄŸi en yÃ¼ksek etkiye sahip.")
        
        if top_feature['direction'] == 'positive':
            narrative_parts.append(f"   â†—ï¸ Bu Ã¶zellik Ã¶tegezegen olasÄ±lÄ±ÄŸÄ±nÄ± artÄ±rÄ±yor.")
        else:
            narrative_parts.append(f"   â†˜ï¸ Bu Ã¶zellik Ã¶tegezegen olasÄ±lÄ±ÄŸÄ±nÄ± azaltÄ±yor.")
        
        # Pozitif ve negatif etkiler
        pos_impact = shap['total_positive_impact']
        neg_impact = abs(shap['total_negative_impact'])
        
        narrative_parts.append(f"\nâš–ï¸ **Etki Dengesi:**")
        narrative_parts.append(f"   â€¢ Pozitif katkÄ±: +{pos_impact:.3f}")
        narrative_parts.append(f"   â€¢ Negatif katkÄ±: {neg_impact:.3f}")
    
    # Karar kurallarÄ±
    if explanation['decision_rules']:
        narrative_parts.append(f"\nğŸ“ **Kritik Ã–zellik Durumu:**")
        for rule in explanation['decision_rules'][:3]:
            status_emoji = "âœ…" if rule['status'] == 'optimal' else "âš ï¸" if rule['status'] == 'acceptable' else "âŒ"
            narrative_parts.append(f"   {status_emoji} {rule['feature']}: {rule['value']:.2f} ({rule['status']})")
    
    # GÃ¼ven faktÃ¶rleri
    if explanation['confidence_factors']:
        conf_factors = explanation['confidence_factors']['factors']
        if conf_factors:
            narrative_parts.append(f"\nğŸ¯ **GÃ¼venilirlik FaktÃ¶rleri:**")
            for factor in conf_factors:
                emoji = "âœ…" if factor['impact'] == 'positive' else "âš ï¸"
                narrative_parts.append(f"   {emoji} {factor['factor']}: {factor['description']}")
    
    return "\n".join(narrative_parts)


def create_shap_waterfall_plotly(shap_values, base_value, features, feature_names, max_display=10):
    """
    Plotly ile interaktif SHAP waterfall grafiÄŸi
    """
    # En etkili Ã¶zellikleri seÃ§
    shap_importance = np.abs(shap_values)
    top_indices = np.argsort(shap_importance)[::-1][:max_display]
    
    # Verileri hazÄ±rla
    selected_shaps = [shap_values[i] for i in top_indices]
    selected_features = [feature_names[i] for i in top_indices]
    selected_values = [features[i] for i in top_indices]
    
    # KÃ¼mÃ¼latif deÄŸerler
    cumulative = [base_value]
    for shap_val in selected_shaps:
        cumulative.append(cumulative[-1] + shap_val)
    
    # Grafik oluÅŸtur
    fig = go.Figure()
    
    # Ã‡ubuklar
    colors = ['red' if s < 0 else 'green' for s in selected_shaps]
    
    for i, (fname, shap_val, color) in enumerate(zip(selected_features, selected_shaps, colors)):
        fig.add_trace(go.Bar(
            x=[shap_val],
            y=[fname],
            orientation='h',
            marker_color=color,
            name=fname,
            text=f"{shap_val:+.3f}",
            textposition='auto',
            hovertemplate=f"<b>{fname}</b><br>SHAP: {shap_val:+.3f}<br>DeÄŸer: {selected_values[i]:.3f}<extra></extra>"
        ))
    
    fig.update_layout(
        title="ğŸŒŠ SHAP Waterfall Plot - Ã–zellik KatkÄ±larÄ±",
        xaxis_title="SHAP DeÄŸeri (Tahmine KatkÄ±)",
        yaxis_title="Ã–zellikler",
        height=500,
        showlegend=False,
        template="plotly_white"
    )
    
    return fig


def create_feature_importance_comparison(model_importance, shap_importance, feature_names, top_n=15):
    """
    Model Ã¶nem derecesi vs SHAP Ã¶nem derecesi karÅŸÄ±laÅŸtÄ±rmasÄ±
    """
    # Veriyi hazÄ±rla
    comparison_df = pd.DataFrame({
        'Feature': feature_names,
        'Model Importance': model_importance,
        'SHAP Importance': shap_importance
    })
    
    # Normalize et
    comparison_df['Model Importance'] = comparison_df['Model Importance'] / comparison_df['Model Importance'].max()
    comparison_df['SHAP Importance'] = comparison_df['SHAP Importance'] / comparison_df['SHAP Importance'].max()
    
    # Top N seÃ§
    comparison_df = comparison_df.nlargest(top_n, 'SHAP Importance')
    
    # Grafik oluÅŸtur
    fig = go.Figure()
    
    fig.add_trace(go.Bar(
        name='Model Ã–nem Derecesi',
        x=comparison_df['Feature'],
        y=comparison_df['Model Importance'],
        marker_color='lightblue'
    ))
    
    fig.add_trace(go.Bar(
        name='SHAP Ã–nem Derecesi',
        x=comparison_df['Feature'],
        y=comparison_df['SHAP Importance'],
        marker_color='orange'
    ))
    
    fig.update_layout(
        title=f"ğŸ“Š Ã–zellik Ã–nem Derecesi KarÅŸÄ±laÅŸtÄ±rmasÄ± (Top {top_n})",
        xaxis_title="Ã–zellikler",
        yaxis_title="Normalize Ã–nem Derecesi",
        barmode='group',
        height=500,
        template="plotly_white",
        xaxis_tickangle=-45
    )
    
    return fig


def create_decision_path_visualization(explanation):
    """
    Karar yolu gÃ¶rselleÅŸtirmesi
    """
    if not explanation.get('decision_rules'):
        return None
    
    rules = explanation['decision_rules']
    
    fig = go.Figure()
    
    for i, rule in enumerate(rules):
        # Durum rengini belirle
        if rule['status'] == 'optimal':
            color = 'green'
        elif rule['status'] == 'acceptable':
            color = 'orange'
        else:
            color = 'red'
        
        # Bar ekle
        fig.add_trace(go.Bar(
            x=[rule['value']],
            y=[rule['feature']],
            orientation='h',
            marker_color=color,
            name=rule['feature'],
            text=f"{rule['value']:.2f}",
            textposition='auto',
            hovertemplate=f"<b>{rule['feature']}</b><br>"
                         f"DeÄŸer: {rule['value']:.2f}<br>"
                         f"Optimal AralÄ±k: {rule['optimal_range'][0]}-{rule['optimal_range'][1]}<br>"
                         f"Durum: {rule['status']}<extra></extra>"
        ))
        
        # Optimal aralÄ±k gÃ¶stergesi
        fig.add_shape(
            type="rect",
            x0=rule['optimal_range'][0],
            y0=i - 0.4,
            x1=rule['optimal_range'][1],
            y1=i + 0.4,
            line=dict(color="green", width=2, dash="dash"),
            fillcolor="lightgreen",
            opacity=0.2,
            layer="below"
        )
    
    fig.update_layout(
        title="ğŸ¯ Kritik Ã–zelliklerin Durum Analizi",
        xaxis_title="DeÄŸer",
        yaxis_title="Ã–zellik",
        height=400,
        showlegend=False,
        template="plotly_white"
    )
    
    return fig


if __name__ == "__main__":
    print("ğŸ§  AI Explainability ModÃ¼lÃ¼")
    print("=" * 60)
    print("Bu modÃ¼l XAI (Explainable AI) yetenekleri saÄŸlar:")
    print("  âœ… SHAP Analizi")
    print("  âœ… LIME AÃ§Ä±klamalarÄ±")
    print("  âœ… Ã–zellik KatkÄ± Analizi")
    print("  âœ… Karar KurallarÄ±")
    print("  âœ… What-If SenaryolarÄ±")
    print("  âœ… GÃ¼ven FaktÃ¶rÃ¼ Analizi")
    print("  âœ… AnlatÄ±sal AÃ§Ä±klamalar")
    print("=" * 60)
