"""
Ã–tegezegen Tespit Sistemi
GerÃ§ek zamanlÄ± NASA verileriyle desteklenen hibrit keÅŸif modeli
"""

import argparse
import os
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import IsolationForest
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, roc_curve
)
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
import warnings
warnings.filterwarnings('ignore')

from data_generator import ExoplanetDataGenerator
from nasa_api import get_latest_dataframe

class ExoplanetDetector:
    """Ã–tegezegen tespit modeli sÄ±nÄ±fÄ±"""
    
    def __init__(self, data_path):
        """
        Parametreler:
            data_path: CSV veri dosyasÄ±nÄ±n yolu
        """
        self.data_path = data_path
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.df = None
        self.anomaly_detector = None
        
    def load_and_explore_data(self, df_override=None, source_name=None):
        """Veriyi yÃ¼kle ve keÅŸfet"""
        print("=" * 80)
        title = "VERÄ° YÃœKLEME VE KEÅÄ°F"
        if source_name:
            title = f"{title} Â· {source_name}"
        print(title)
        print("=" * 80)

        if df_override is not None:
            self.df = df_override.copy()
        else:
            # CSV dosyasÄ±nÄ± yÃ¼kle (yorum satÄ±rlarÄ±nÄ± atla)
            self.df = pd.read_csv(self.data_path, comment='#')
        
        print(f"\nğŸ“Š Toplam veri noktasÄ±: {len(self.df)}")
        print(f"ğŸ“Š Toplam Ã¶zellik sayÄ±sÄ±: {len(self.df.columns)}")
        
        # Hedef deÄŸiÅŸkeni kontrol et
        if 'koi_disposition' in self.df.columns:
            print(f"\nğŸ¯ Hedef deÄŸiÅŸken daÄŸÄ±lÄ±mÄ±:")
            print(self.df['koi_disposition'].value_counts())
            print(f"\nYÃ¼zdeler:")
            print(self.df['koi_disposition'].value_counts(normalize=True) * 100)
        
        # Eksik veri analizi
        print(f"\nâŒ Eksik veri oranlarÄ± (>50% olanlar):")
        missing = (self.df.isnull().sum() / len(self.df)) * 100
        high_missing = missing[missing > 50].sort_values(ascending=False)
        if len(high_missing) > 0:
            print(high_missing)
        else:
            print("50%'den fazla eksik veriye sahip sÃ¼tun yok")
        
        return self.df

    def load_live_data(self, mission='kepler', limit=500, force_refresh=False):
        """NASA Exoplanet Archive'dan canlÄ± veri Ã§ek"""
        print("=" * 80)
        print(f"NASA EXOPLANET API ({mission.upper()})")
        print("=" * 80)

        live_df = get_latest_dataframe(mission=mission, limit=limit, force_refresh=force_refresh)
        print(f"\nğŸ“¡ {mission.upper()} kataloÄŸundan {len(live_df)} kayÄ±t alÄ±ndÄ±")
        print(f"ğŸ“… Son gÃ¼ncelleme iÃ§in {limit} kayÄ±t sÄ±nÄ±rÄ± kullanÄ±ldÄ±")

        self.df = live_df
        return self.df
    
    def preprocess_data(self):
        """Veriyi Ã¶n iÅŸle ve Ã¶zellik mÃ¼hendisliÄŸi yap"""
        print("\n" + "=" * 80)
        print("VERÄ° Ã–N Ä°ÅLEME VE Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ°")
        print("=" * 80)
        
        # Hedef deÄŸiÅŸkeni oluÅŸtur (CONFIRMED = 1, diÄŸerleri = 0)
        self.df['is_exoplanet'] = (self.df['koi_disposition'] == 'CONFIRMED').astype(int)
        
        # En Ã¶nemli Ã¶zellikleri seÃ§
        important_features = [
            'koi_period',           # YÃ¶rÃ¼nge periyodu
            'koi_depth',            # GeÃ§iÅŸ derinliÄŸi
            'koi_duration',         # GeÃ§iÅŸ sÃ¼resi
            'koi_ror',              # Gezegen-yÄ±ldÄ±z yarÄ±Ã§ap oranÄ±
            'koi_srho',             # YÄ±ldÄ±z yoÄŸunluÄŸu
            'koi_prad',             # Gezegen yarÄ±Ã§apÄ±
            'koi_teq',              # Denge sÄ±caklÄ±ÄŸÄ±
            'koi_insol',            # GÃ¼neÅŸ Ä±ÅŸÄ±nÄ±mÄ±
            'koi_steff',            # YÄ±ldÄ±z sÄ±caklÄ±ÄŸÄ±
            'koi_slogg',            # YÄ±ldÄ±z yÃ¼zey yerÃ§ekimi
            'koi_srad',             # YÄ±ldÄ±z yarÄ±Ã§apÄ±
            'koi_impact',           # Etki parametresi
            'koi_model_snr',        # Sinyal-gÃ¼rÃ¼ltÃ¼ oranÄ±
            'koi_tce_plnt_num',     # Gezegen numarasÄ±
            'koi_fpflag_nt',        # YanlÄ±ÅŸ pozitif bayraklarÄ±
            'koi_fpflag_ss',
            'koi_fpflag_co',
            'koi_fpflag_ec',
        ]
        
        # Mevcut Ã¶zellikleri filtrele
        available_features = [f for f in important_features if f in self.df.columns]
        print(f"âœ… KullanÄ±lacak Ã¶zellikler ({len(available_features)}):")
        for feat in available_features:
            print(f"  - {feat}")
        
        # Ã–zellikleri ve hedef deÄŸiÅŸkeni ayÄ±r
        X = self.df[available_features].copy()
        y = self.df['is_exoplanet'].copy()
        
        # Eksik deÄŸerleri doldur
        print(f"\nğŸ”§ Eksik deÄŸerler medyan ile doldruluyor...")
        X = X.fillna(X.median())
        
        # Sonsuz deÄŸerleri temizle
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        # Ã–zellik mÃ¼hendisliÄŸi
        print(f"\nâš™ï¸ Yeni Ã¶zellikler oluÅŸturuluyor...")
        
        # 1. Gezegen-yÄ±ldÄ±z boyut oranÄ±
        if 'koi_prad' in X.columns and 'koi_srad' in X.columns:
            X['planet_star_ratio'] = X['koi_prad'] / (X['koi_srad'] * 109.2)  # GÃ¼neÅŸ yarÄ±Ã§apÄ± -> DÃ¼nya yarÄ±Ã§apÄ±
        
        # 2. Sinyal kalitesi gÃ¶stergesi
        if 'koi_depth' in X.columns and 'koi_model_snr' in X.columns:
            X['signal_quality'] = X['koi_depth'] * X['koi_model_snr']
        
        # 3. YÃ¶rÃ¼nge hÄ±zÄ± tahmini
        if 'koi_period' in X.columns:
            X['orbital_velocity'] = 1 / X['koi_period']
        
        # 4. YanlÄ±ÅŸ pozitif toplam skoru
        fp_flags = ['koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec']
        available_fp_flags = [f for f in fp_flags if f in X.columns]
        if available_fp_flags:
            X['fp_total_score'] = X[available_fp_flags].sum(axis=1)
        
        # 5. GeÃ§iÅŸ ÅŸekil faktÃ¶rÃ¼
        if 'koi_duration' in X.columns and 'koi_period' in X.columns:
            X['transit_shape_factor'] = X['koi_duration'] / X['koi_period']
        
        # Son temizlik
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        self.feature_names = X.columns.tolist()
        print(f"\nâœ¨ Toplam Ã¶zellik sayÄ±sÄ±: {len(self.feature_names)}")
        
        return X, y

    def generate_synthetic_dataset(self, X, y, n_samples=2000, strategy='hybrid', output_path='synthetic_exoplanets.csv'):
        """Sentetik veri Ã¼ret ve CSV olarak kaydet"""
        print("\n" + "=" * 80)
        print("SENTETÄ°K VERÄ° ÃœRETÄ°MÄ°")
        print("=" * 80)

        generator = ExoplanetDataGenerator(random_state=42)
        generator.fit(X, y)
        synthetic_df = generator.generate_dataset(
            n_samples=n_samples,
            strategy=strategy,
            include_labels=True
        )
        synthetic_df.to_csv(output_path, index=False)

        print(f"âœ… {len(synthetic_df)} satÄ±rlÄ±k yeni veri Ã¼retildi ve '{output_path}' dosyasÄ±na kaydedildi")
        class_breakdown = synthetic_df['is_exoplanet'].value_counts()
        for label, count in class_breakdown.items():
            ratio = count / len(synthetic_df) * 100
            print(f"  - SÄ±nÄ±f {label}: {count} (%{ratio:.2f})")

        return synthetic_df
    
    def balance_dataset(self, X, y):
        """Dengesiz veri setini dengele (SMOTE + Undersampling)"""
        print("\n" + "=" * 80)
        print("VERÄ° DENGELEme")
        print("=" * 80)
        
        print(f"ğŸ“Š Dengeleme Ã¶ncesi daÄŸÄ±lÄ±m:")
        print(f"  - Ã–tegezegen deÄŸil (0): {(y == 0).sum()}")
        print(f"  - Ã–tegezegen (1): {(y == 1).sum()}")
        print(f"  - Dengesizlik oranÄ±: {(y == 0).sum() / (y == 1).sum():.2f}:1")
        
        # SMOTE + Random Undersampling kombinasyonu
        over_sampler = SMOTE(sampling_strategy=0.5, random_state=42)
        under_sampler = RandomUnderSampler(sampling_strategy=0.8, random_state=42)
        
        X_balanced, y_balanced = over_sampler.fit_resample(X, y)
        X_balanced, y_balanced = under_sampler.fit_resample(X_balanced, y_balanced)
        
        print(f"\nğŸ“Š Dengeleme sonrasÄ± daÄŸÄ±lÄ±m:")
        print(f"  - Ã–tegezegen deÄŸil (0): {(y_balanced == 0).sum()}")
        print(f"  - Ã–tegezegen (1): {(y_balanced == 1).sum()}")
        print(f"  - Yeni dengesizlik oranÄ±: {(y_balanced == 0).sum() / (y_balanced == 1).sum():.2f}:1")
        
        return X_balanced, y_balanced
    
    def train_model(self, X_train, y_train):
        """XGBoost modelini eÄŸit"""
        print("\n" + "=" * 80)
        print("MODEL EÄÄ°TÄ°MÄ°")
        print("=" * 80)
        
        # Veriyi Ã¶lÃ§eklendir
        print("ğŸ”§ Veriler Ã¶lÃ§eklendiriliyor (Robust Scaler)...")
        self.scaler = RobustScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # XGBoost modeli - optimize edilmiÅŸ parametreler
        print("ğŸ¤– XGBoost modeli oluÅŸturuluyor...")
        self.model = XGBClassifier(
            n_estimators=300,
            max_depth=7,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            gamma=1,
            min_child_weight=3,
            reg_alpha=0.1,
            reg_lambda=1,
            scale_pos_weight=1,
            random_state=42,
            n_jobs=-1,
            eval_metric='logloss'
        )
        
        print("ğŸ“ Model eÄŸitiliyor...")
        self.model.fit(
            X_train_scaled, 
            y_train,
            verbose=False
        )
        
        print("ğŸ›° Anomali dedektÃ¶rÃ¼ hazÄ±rlanÄ±yor (IsolationForest)...")
        self.anomaly_detector = IsolationForest(
            n_estimators=256,
            contamination=0.02,
            random_state=42,
            n_jobs=-1,
        )
        self.anomaly_detector.fit(X_train_scaled)

        print("âœ… Model eÄŸitimi tamamlandÄ±!")
        
        return self.model
    
    def evaluate_model(self, X_test, y_test):
        """Modeli deÄŸerlendir ve metrikleri gÃ¶ster"""
        print("\n" + "=" * 80)
        print("MODEL DEÄERLENDÄ°RME")
        print("=" * 80)
        
        # Tahmin yap
        X_test_scaled = self.scaler.transform(X_test)
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        # Temel metrikler
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        print(f"\nğŸ“ˆ PERFORMANS METRÄ°KLERÄ°:")
        print(f"  - DoÄŸruluk (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"  - Kesinlik (Precision): {precision:.4f} ({precision*100:.2f}%)")
        print(f"  - DuyarlÄ±lÄ±k (Recall): {recall:.4f} ({recall*100:.2f}%)")
        print(f"  - F1 Skoru: {f1:.4f}")
        print(f"  - ROC AUC: {roc_auc:.4f}")
        
        # KarmaÅŸÄ±klÄ±k matrisi
        cm = confusion_matrix(y_test, y_pred)
        print(f"\nğŸ“Š KARMAÅIKLIK MATRÄ°SÄ°:")
        print(f"  - DoÄŸru Negatif (TN): {cm[0,0]}")
        print(f"  - YanlÄ±ÅŸ Pozitif (FP): {cm[0,1]}")
        print(f"  - YanlÄ±ÅŸ Negatif (FN): {cm[1,0]}")
        print(f"  - DoÄŸru Pozitif (TP): {cm[1,1]}")
        
        # DetaylÄ± rapor
        print(f"\nğŸ“‹ DETAYLI SINIFLANDIRMA RAPORU:")
        print(classification_report(y_test, y_pred, 
                                   target_names=['Ã–tegezegen DeÄŸil', 'Ã–tegezegen']))
        
        # Ã–zellik Ã¶nem dereceleri
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nâ­ EN Ã–NEMLÄ° 10 Ã–ZELLÄ°K:")
        for idx, row in feature_importance.head(10).iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")
        
        novelty_scores = None
        novelty_flags = None
        if self.anomaly_detector is not None:
            novelty_scores = self.anomaly_detector.decision_function(X_test_scaled)
            novelty_flags = self.anomaly_detector.predict(X_test_scaled)
            # IsolationForest returns -1 for anomaly
            novel_count = int((novelty_flags == -1).sum())
            print(f"\nğŸ§­ Potansiyel yeni aday sayÄ±sÄ±: {novel_count}")

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'confusion_matrix': cm,
            'feature_importance': feature_importance,
            'novelty_scores': novelty_scores,
            'novelty_flags': novelty_flags
        }
    
    def save_model(self, model_path='exoplanet_model.pkl', scaler_path='scaler.pkl'):
        """Modeli ve scaler'Ä± kaydet"""
        joblib.dump(self.model, model_path)
        joblib.dump(self.scaler, scaler_path)
        joblib.dump(self.feature_names, 'feature_names.pkl')
        if self.anomaly_detector is not None:
            joblib.dump(self.anomaly_detector, 'anomaly_detector.pkl')
        print(f"\nğŸ’¾ Model kaydedildi: {model_path}")
        print(f"ğŸ’¾ Scaler kaydedildi: {scaler_path}")
        print(f"ğŸ’¾ Ã–zellik isimleri kaydedildi: feature_names.pkl")
        if self.anomaly_detector is not None:
            print("ğŸ’¾ Anomali modeli kaydedildi: anomaly_detector.pkl")
    
    def load_model(self, model_path='exoplanet_model.pkl', scaler_path='scaler.pkl'):
        """KaydedilmiÅŸ modeli yÃ¼kle"""
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)
        self.feature_names = joblib.load('feature_names.pkl')
        anomaly_path = Path('anomaly_detector.pkl')
        if anomaly_path.exists():
            self.anomaly_detector = joblib.load(anomaly_path)
        print(f"âœ… Model yÃ¼klendi: {model_path}")
    
    def predict_single(self, features_dict):
        """Tek bir veri noktasÄ± iÃ§in tahmin yap"""
        # Ã–zellikleri DataFrame'e Ã§evir
        X = pd.DataFrame([features_dict])
        
        # Eksik Ã¶zellikleri 0 ile doldur
        for feature in self.feature_names:
            if feature not in X.columns:
                X[feature] = 0
        
        # Ã–zellikleri doÄŸru sÄ±rada al
        X = X[self.feature_names]
        
        # Ã–lÃ§eklendir ve tahmin yap
        X_scaled = self.scaler.transform(X)
        prediction = self.model.predict(X_scaled)[0]
        probability = self.model.predict_proba(X_scaled)[0]
        novelty = None
        if self.anomaly_detector is not None:
            novelty_flag = self.anomaly_detector.predict(X_scaled)[0]
            novelty_score = self.anomaly_detector.decision_function(X_scaled)[0]
            novelty = {
                'is_novel_candidate': bool(novelty_flag == -1),
                'novelty_score': float(novelty_score)
            }

        return {
            'is_exoplanet': bool(prediction),
            'probability_not_exoplanet': float(probability[0]),
            'probability_exoplanet': float(probability[1]),
            'confidence': float(max(probability)),
            'novelty': novelty
        }

    def detect_novel_candidates(self, X):
        """Veri kÃ¼mesindeki potansiyel yeni adaylarÄ± dÃ¶ndÃ¼r"""
        if self.anomaly_detector is None:
            raise RuntimeError("Anomali modeli yÃ¼klenmeden bu fonksiyon Ã§aÄŸrÄ±lamaz")

        X_local = X.copy()
        missing_cols = [col for col in self.feature_names if col not in X_local.columns]
        for col in missing_cols:
            X_local[col] = 0
        X_local = X_local[self.feature_names]

        X_scaled = self.scaler.transform(X_local)
        novelty_flags = self.anomaly_detector.predict(X_scaled)
        novelty_scores = self.anomaly_detector.decision_function(X_scaled)

        result = X_local.copy()
        result['novelty_score'] = novelty_scores
        result['is_novel_candidate'] = novelty_flags == -1
        return result[result['is_novel_candidate']]
    
    def plot_results(self, metrics):
        """SonuÃ§larÄ± gÃ¶rselleÅŸtir"""
        print("\nğŸ“Š Grafikler oluÅŸturuluyor...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. KarmaÅŸÄ±klÄ±k Matrisi
        cm = metrics['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_title('KarmaÅŸÄ±klÄ±k Matrisi')
        axes[0, 0].set_xlabel('Tahmin')
        axes[0, 0].set_ylabel('GerÃ§ek')
        axes[0, 0].set_xticklabels(['DeÄŸil', 'Ã–tegezegen'])
        axes[0, 0].set_yticklabels(['DeÄŸil', 'Ã–tegezegen'])
        
        # 2. Metrik KarÅŸÄ±laÅŸtÄ±rmasÄ±
        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']
        metrics_values = [
            metrics['accuracy'],
            metrics['precision'],
            metrics['recall'],
            metrics['f1'],
            metrics['roc_auc']
        ]
        axes[0, 1].bar(metrics_names, metrics_values, color='skyblue')
        axes[0, 1].set_title('Model Performans Metrikleri')
        axes[0, 1].set_ylim([0, 1])
        axes[0, 1].axhline(y=0.9, color='r', linestyle='--', label='%90 Hedef')
        for i, v in enumerate(metrics_values):
            axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')
        axes[0, 1].legend()
        
        # 3. Ã–zellik Ã–nem Dereceleri (Top 15)
        top_features = metrics['feature_importance'].head(15)
        axes[1, 0].barh(top_features['feature'], top_features['importance'], color='coral')
        axes[1, 0].set_title('En Ã–nemli 15 Ã–zellik')
        axes[1, 0].set_xlabel('Ã–nem Derecesi')
        axes[1, 0].invert_yaxis()
        
        # 4. Ã–zellik Ã–nem DaÄŸÄ±lÄ±mÄ±
        axes[1, 1].hist(metrics['feature_importance']['importance'], bins=30, 
                       color='lightgreen', edgecolor='black')
        axes[1, 1].set_title('Ã–zellik Ã–nem Derecesi DaÄŸÄ±lÄ±mÄ±')
        axes[1, 1].set_xlabel('Ã–nem Derecesi')
        axes[1, 1].set_ylabel('Frekans')
        
        plt.tight_layout()
        plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')
        print("âœ… Grafikler 'model_performance.png' olarak kaydedildi")
        plt.show()


def main():
    """Ana program"""
    parser = argparse.ArgumentParser(
        description="Ã–tegezegen tespit modelini eÄŸit ve deÄŸerlendir"
    )
    parser.add_argument(
        "--source",
        choices=["local", "live", "hybrid"],
        default="local",
        help="Veri kaynaÄŸÄ±nÄ± seÃ§ (yerel CSV, canlÄ± NASA verisi veya hibrit)"
    )
    parser.add_argument(
        "--mission",
        choices=["kepler"],
        default="kepler",
        help="CanlÄ± veri iÃ§in NASA gÃ¶revi (yalnÄ±zca Kepler desteklenir)"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=500,
        help="NASA API'dan Ã§ekilecek maksimum kayÄ±t sayÄ±sÄ±"
    )
    parser.add_argument(
        "--force-refresh",
        action="store_true",
        help="NASA API Ã¶nbelleÄŸini zorla yenile"
    )
    parser.add_argument(
        "--data-path",
        default='cumulative_2025.10.04_09.55.40.csv',
        help="Yerel Kepler CSV veri dosyasÄ±nÄ±n yolu"
    )
    parser.add_argument(
        "--skip-synthetic",
        action="store_true",
        help="Sentetik veri Ã¼retimini atla"
    )
    parser.add_argument(
        "--synthetic-samples",
        type=int,
        default=3000,
        help="Ãœretilecek sentetik Ã¶rnek sayÄ±sÄ±"
    )
    parser.add_argument(
        "--synthetic-strategy",
        choices=["hybrid", "positive", "negative", "physical"],
        default="hybrid",
        help="Sentetik veri Ã¼retim stratejisi"
    )
    args = parser.parse_args()

    print("\n" + "=" * 80)
    print("ğŸŒŒ Ã–TEGEZEGEN KEÅFETME SÄ°STEMÄ° ğŸŒŒ")
    print("NASA Kepler Veri Seti ile Makine Ã–ÄŸrenimi")
    print("=" * 80 + "\n")

    # Veri dosyasÄ± yolu
    data_path = args.data_path

    # Detector'Ä± oluÅŸtur
    detector = ExoplanetDetector(data_path)

    # 1. Veriyi yÃ¼kle ve keÅŸfet
    if args.source == "local":
        df = detector.load_and_explore_data(source_name="Yerel CSV")
    elif args.source == "live":
        live_df = detector.load_live_data(
            mission=args.mission,
            limit=args.limit,
            force_refresh=args.force_refresh
        )
        df = detector.load_and_explore_data(
            df_override=live_df,
            source_name=f"NASA {args.mission.upper()} canlÄ± verisi"
        )
    else:
        # Hibrit veri seti: yerel + NASA canlÄ± verisi
        print("ğŸŒ Hibrit veri hazÄ±rlÄ±ÄŸÄ± baÅŸlatÄ±lÄ±yor...")
        live_df = detector.load_live_data(
            mission=args.mission,
            limit=args.limit,
            force_refresh=args.force_refresh
        )
        local_df = pd.read_csv(data_path, comment='#')
        combined_df = pd.concat([local_df, live_df], ignore_index=True, sort=False)
        dedupe_keys = [col for col in ['kepid', 'koi_name', 'tic_id', 'planet_name'] if col in combined_df.columns]
        if dedupe_keys:
            combined_df = combined_df.drop_duplicates(subset=dedupe_keys, keep='last')
        else:
            combined_df = combined_df.drop_duplicates()
        combined_df = combined_df.reset_index(drop=True)
        print(f"ğŸ§¬ Hibrit veri seti oluÅŸturuldu: {len(local_df)} yerel + {len(live_df)} canlÄ± kayÄ±t")
        df = detector.load_and_explore_data(
            df_override=combined_df,
            source_name=f"Hibrit ({args.mission.upper()} + yerel)"
        )
    
    # 2. Veriyi Ã¶n iÅŸle
    X, y = detector.preprocess_data()

    # 2.1 Sentetik veri Ã¼ret
    synthetic_df = None
    if args.skip_synthetic or args.synthetic_samples <= 0:
        print("â­ Sentetik veri Ã¼retimi kullanÄ±cÄ± tercihiyle atlandÄ±.")
    else:
        synthetic_df = detector.generate_synthetic_dataset(
            X,
            y,
            n_samples=args.synthetic_samples,
            strategy=args.synthetic_strategy
        )
    
    # 3. Train-test split
    print("\n" + "=" * 80)
    print("VERÄ° BÃ–LÃœNMESÄ°")
    print("=" * 80)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"âœ… EÄŸitim seti: {len(X_train)} Ã¶rnek")
    print(f"âœ… Test seti: {len(X_test)} Ã¶rnek")
    
    # 4. Veriyi dengele
    X_train_balanced, y_train_balanced = detector.balance_dataset(X_train, y_train)
    
    # 5. Modeli eÄŸit
    detector.train_model(X_train_balanced, y_train_balanced)
    
    # 6. Modeli deÄŸerlendir
    metrics = detector.evaluate_model(X_test, y_test)
    
    # 7. Modeli kaydet
    detector.save_model()
    
    # 8. SonuÃ§larÄ± gÃ¶rselleÅŸtir
    detector.plot_results(metrics)
    
    # 9. Ã–rnek tahmin
    print("\n" + "=" * 80)
    print("Ã–RNEK TAHMÄ°N")
    print("=" * 80)
    print("\nğŸ”® Test setinden rastgele bir Ã¶rnek seÃ§iliyor...")
    
    sample_idx = np.random.randint(0, len(X_test))
    sample_features = X_test.iloc[sample_idx].to_dict()
    true_label = y_test.iloc[sample_idx]
    
    prediction = detector.predict_single(sample_features)
    
    print(f"\nğŸ“Š GerÃ§ek Durum: {'âœ… Ã–TEGEZEGEN' if true_label == 1 else 'âŒ Ã–TEGEZEGEN DEÄÄ°L'}")
    print(f"ğŸ¤– Model Tahmini: {'âœ… Ã–TEGEZEGEN' if prediction['is_exoplanet'] else 'âŒ Ã–TEGEZEGEN DEÄÄ°L'}")
    print(f"ğŸ“ˆ Ã–tegezegen Olma OlasÄ±lÄ±ÄŸÄ±: {prediction['probability_exoplanet']*100:.2f}%")
    print(f"ğŸ“ˆ GÃ¼ven Skoru: {prediction['confidence']*100:.2f}%")
    
    print("\n" + "=" * 80)
    print("âœ¨ Program tamamlandÄ±! âœ¨")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
